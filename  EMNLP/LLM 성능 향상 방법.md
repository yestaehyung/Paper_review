---
title: "LLM 성능 향상 방법"
source: "https://chatgpt.com/c/67f32144-d55c-800a-830b-7b9ef6f821a9"
author:
  - "[[ChatGPT]]"
published:
created: 2025-04-07
description: "ChatGPT conversation with 4 turns"
tags:
  - "clippings"
---
## Refinement Methods for Improving LLM Outputs (NeurIPS 2023–2024)

The table below summarizes several recent research papers (mostly NeurIPS) that propose **refinement approaches** for LLM-generated outputs, along with their methods and performance gains:

| **Paper**                                                                                  | **Authors (Year)**                                | **Refinement Approach**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | **Key Outcomes**                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Self-Refine: Iterative Refinement with Self-Feedback**                                   | Aman Madaan *et al.* (NeurIPS 2023)               | Uses the same LLM to generate an initial output, then have the LLM itself provide feedback on that output and iteratively refine it – all at test-time, with no additional training or supervised data required [^1].                                                                                                                                                                                                                                                                                                                   | Across 7 diverse tasks (dialog generation, math reasoning, etc.), this method improved performance by **~20% (absolute)** on average over one-shot generation, with refined outputs preferred by both automatic metrics and human evaluators [^2].                                                                                                                                                                                                                                          |
| **Reflexion: Language Agents with Verbal Reinforcement Learning**                          | Noah Shinn *et al.* (NeurIPS 2023)                | Introduces *Reflexion*, where an agent doesn’t update its weights but **verbally self-reflects** on feedback. The LLM agent converts feedback signals (e.g. error messages or critique) into a textual reflection stored in memory, and conditions future outputs on this memory to iteratively improve decisions [^3].                                                                                                                                                                                                                 | Achieved **significant improvements** across reasoning, decision-making, and coding tasks. For example, on the HumanEval code benchmark it reached **91% pass@1 accuracy**, outperforming GPT-4 (which achieved ~80%) [^4].                                                                                                                                                                                                                                                                 |
| **LeDex: Training LLMs to Better Self-Debug and Explain Code**                             | Nan Jiang *et al.* (NeurIPS 2024)                 | Proposes a training framework for code generation where the model **iteratively debugs its code**. LeDex automatically builds a dataset of code “explanations and refinements” by generating multiple explanation-and-fix trajectories (using the LLM itself or a larger teacher) and filtering them via execution results [^5]. The model is then fine-tuned on these successful and failed refinement trajectories (with a specially designed reward for good explanations and fixes), and further optimized with RL.                 | This approach markedly improved code correctness: supervised fine-tuning alone boosted the code pass@1 rate by up to **+15.9%** (and pass@10 by +9.3%) across several benchmarks, and an additional RL fine-tuning stage added up to **+3.5%** more on pass@1 (and +2.5% on pass@10) [^6]. The resulting LLM can continuously refine its code and produces more helpful explanations of bugs (as confirmed by human evaluation) [^6].                                                       |
| **SLED: Self Logits Evolution Decoding for Improving Factuality in LLMs**                  | Jianyi Zhang *et al.* (NeurIPS 2024)              | Introduces a novel **decoding-time refinement** method to improve factual accuracy. SLED leverages the model’s *latent knowledge* by contrasting the final-layer output logits with the logits from earlier transformer layers, and then applying an approximate gradient-based adjustment to **self-refine** the output in real-time [^7]. This guides the generation to be more consistent with facts the model *does* know internally, without any external knowledge or fine-tuning.                                                | **Consistently improves factual accuracy** (truthfulness of outputs) by up to **20%** compared to standard decoding methods, while maintaining fluent language quality [^8]. Notably, SLED operates with negligible added latency and can be combined with other decoding strategies for further gains [^8].                                                                                                                                                                                |
| **Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing (AlphaLLM)** | Ye Tian *et al.* (NeurIPS 2024)                   | Proposes **AlphaLLM**, which integrates a Monte Carlo Tree Search (MCTS) loop with an LLM for complex reasoning tasks. The LLM “imagines” possible solution steps, uses MCTS to **search** multiple reasoning trajectories, and employs a set of critic models to **critique** and score these trajectories [^9]. High-scoring solutions are then used to update the LLM’s own responses (in a self-improvement loop) – all without additional human-provided annotations.                                                              | Demonstrated **substantial performance gains** on mathematical reasoning benchmarks. For instance, starting from a strong 70B model (~73.7% accuracy on GSM8K math problems), AlphaLLM’s search-and-refine loop boosted accuracy to **88.9%**, and after two self-improvement iterations it reached **~92%**, **rivaling GPT-4’s performance** on the same task [^10]. Similar improvements were observed on other math tasks (e.g. more than doubling accuracy on the MATH dataset) [^10]. |
| **Code Repair with LLMs Gives an Exploration-Exploitation Tradeoff (REx)**                 | Hao Tang *et al.* (NeurIPS 2024)                  | Focuses on **iterative code refinement** using feedback from test cases. The authors frame the process as a **bandit problem**: at each step, the algorithm must decide whether to *exploit* the current best partial solution or *explore* a different solution path [^11]. They introduce **REx**, which uses Thompson Sampling to adaptively choose which generated program to refine next, rather than always picking the last or best one. This balances exploration vs. exploitation in the refinement search tree of code edits. | REx solves **more programming problems with fewer LLM calls** compared to greedy or breadth-first refinement strategies [^12]. It achieved new **state-of-the-art results** in several challenging code generation domains (such as loop invariant synthesis, visual reasoning puzzles, and competition-level programming tasks) [^13], indicating a more efficient and effective refinement strategy.                                                                                      |
| **LLM Self-Correction with DeCRIM (Decompose, Critique, and Refine)**                      | Thomas P. Ferraz *et al.* (NeurIPS 2024 Workshop) | Targets **instruction-following with multiple constraints** (e.g. “write a post in a *funny* tone *without* using hashtags”). The DeCRIM pipeline first **decomposes** a complex instruction into a checklist of individual constraints. A separate *Critic* model then evaluates the LLM’s output to see if any constraint is violated and pinpoints where [^14]. The LLM subsequently attempts to **refine** its answer to fix those specific issues, iterating as needed.                                                            | On the new RealInstruct benchmark of real user queries with multiple constraints, DeCRIM boosted an open-source model’s success rate by **+7.3%** (and by +8.0% on a related IFEval benchmark) even with only weak automated feedback [^15]. With stronger feedback signals, a DeCRIM-augmented LLM was able to **exceed GPT-4’s performance** on following complex constrained instructions [^15], substantially closing the gap between open models and proprietary ones in this setting. |

[^1]: [nips.cc](https://nips.cc/virtual/2023/poster/71632#:~:text=text%2C%20we%20introduce%20Self,3.5) — text...

[^2]: [nips.cc](https://nips.cc/virtual/2023/poster/71632#:~:text=response%20generation%20to%20mathematical%20reasoning%2C,using%20our%20simple%2C%20standalone%20approach) — response generation to mathematical reasoning...

[^3]: [neurips.cc](https://neurips.cc/virtual/2023/poster/70114#:~:text=propose%20%5Cemph,making%20in) — propose \\emph...

[^4]: [neurips.cc](https://neurips.cc/virtual/2023/poster/70114#:~:text=match%20at%20L236%20language%20reasoning%29,methods%2C%20and%20agent%20types%2C%20and) — match at L236 language reasoning)...

[^5]: [neurips.cc](https://neurips.cc/virtual/2024/poster/94367#:~:text=open,and%20failure%20trajectories%20with%20a) — open...

[^6]: [neurips.cc](https://neurips.cc/virtual/2024/poster/94367#:~:text=reinforcement%20learning%20,understand%20bugs%20in%20source%20code) — reinforcement learning...

[^7]: [proceedings.neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2024/hash/0939f13ffce3ff487509d902ddba4571-Abstract-Conference.html#:~:text=this%2C%20we%20introduce%20Self%20Logits,LLaMA%202%2C%20LLaMA) — this...

[^8]: [proceedings.neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2024/hash/0939f13ffce3ff487509d902ddba4571-Abstract-Conference.html#:~:text=configurations%20such%20as%20the%20mixture,fluency%20and%20negligible%20latency%20overhead) — configurations such as the mixture...

[^9]: [proceedings.neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2024/hash/5e5853f35164e434015716a8c2a66543-Abstract-Conference.html#:~:text=employing%20strategies%20that%20allow%20LLMs,language%20tasks%2C%20and%20the%20subjective) — employing strategies that allow LLMs...

[^10]: [proceedings.neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2024/file/5e5853f35164e434015716a8c2a66543-Paper-Conference.pdf#:~:text=imagination,solving%20tasks%20in%20a) — imagination...

[^11]: [haotang1995.github.io](https://haotang1995.github.io/projects/rex#:~:text=iteratively%20refine%20code%2C%20with%20prior,solve%20more%20problems%20using%20fewer) — iteratively refine code...

[^12]: [haotang1995.github.io](https://haotang1995.github.io/projects/rex#:~:text=acquiring%20bandit%20problem%2C%20which%20we,using%20fewer%20language%20model%20calls) — acquiring bandit problem...

[^13]: [haotang1995.github.io](https://haotang1995.github.io/projects/rex#:~:text=Image%20REx%20established%20new%20state,three%20challenging%20code%20generation%20domains) — Image REx established new state...

[^14]: [neurips.cc](https://neurips.cc/virtual/2024/104307#:~:text=and%20proprietary%20models%2C%20we%20propose,3) — and proprietary models...

[^15]: [neurips.cc](https://neurips.cc/virtual/2024/104307#:~:text=DeCRIM%20works%20by%20decomposing%20the,4%20on%20both%20benchmarks) — DeCRIM works by decomposing the...
Speculative decoding

디코딩을 빨리하고자함 
모델을 두개 가지고 있음
- target model - main llm
- draft model - 작고 가벼운 llm
두 개는 같은 tokenizer를 사용해야함

단어를 예측할 때, 작은 모델로 일단 뱉음 -> 이걸 큰 모델이 다시 확률 계산을 해서 더 확률이 높은 토큰으로 교체를 하는 개념